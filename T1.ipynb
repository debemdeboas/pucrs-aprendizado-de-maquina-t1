{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado de Máquina - Trabalho 1\n",
    "\n",
    "Alunos: Alison Schemitt, Leonardo Faé, Rafael Almeida de Bem, Renata Delia,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Annotated\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from numpy import ScalarType\n",
    "from numpy.typing import ArrayLike\n",
    "# Import only classification datasets\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "datasets = [load_iris(return_X_y=True), load_wine(return_X_y=True)]\n",
    "datasets_names = [\"iris\", \"wine\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código adaptado do GPT-4.\n",
    "# Modificações:\n",
    "# - Tipagem de parâmetros e retorno\n",
    "def train_test_split_gpt(df: pd.DataFrame, test_size: int | float = 0.2) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split the DataFrame into training and testing sets.\"\"\"\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def multiclass_roc_auc_score_gpt(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "def get_generic_metrics(author: str, model: str, dataset: str, y_pred, y_test, _roc_auc_score, _print: bool = False) -> dict:\n",
    "    if _roc_auc_score == None:\n",
    "        return {\n",
    "            'author': author,\n",
    "            'model': model,\n",
    "            'dataset': dataset,\n",
    "        }\n",
    "    \n",
    "    if _print:\n",
    "        print(f'{author} {model}: Dataset {dataset}')\n",
    "        print(f'Accuracy: {accuracy_score(y_test, y_pred):.03}')\n",
    "        print(f'F1: {f1_score(y_test, y_pred, average=\"macro\"):.03}')\n",
    "        print(f'Precision: {precision_score(y_test, y_pred, average=\"macro\"):.03}')\n",
    "        print(f'Recall: {recall_score(y_test, y_pred, average=\"macro\"):.03}')\n",
    "        print(f'ROC AUC score: {_roc_auc_score:.03}')\n",
    "\n",
    "    return {\n",
    "        'author': author,\n",
    "        'model': model,\n",
    "        'dataset': dataset,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, average=\"macro\"),\n",
    "        'precision': precision_score(y_test, y_pred, average=\"macro\"),\n",
    "        'recall': recall_score(y_test, y_pred, average=\"macro\"),\n",
    "        'roc_auc': _roc_auc_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt: Act as a machine learning engineer. Write a Python script that only uses NumPy and Pandas and implements the Decision Tree machine learning algorithm for classify a new instance of data based in a dataset. The code should be commented, complete, and generic.\n",
    "\n",
    "GPT - 4:\n",
    "Here is a sample implementation of the Decision Tree algorithm using NumPy and Pandas libraries in Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or n_classes == 1 or n_samples < 2):\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        # Find best split\n",
    "        best_feature, best_threshold = self.find_best_split(X, y)\n",
    "\n",
    "        # Split data\n",
    "        left_idxs = X[:, best_feature] < best_threshold\n",
    "        right_idxs = ~left_idxs\n",
    "        left_tree = self.build_tree(X[left_idxs], y[left_idxs], depth+1)\n",
    "        right_tree = self.build_tree(X[right_idxs], y[right_idxs], depth+1)\n",
    "\n",
    "        # Create node\n",
    "        node = {'feature': best_feature,\n",
    "                'threshold': best_threshold,\n",
    "                'left': left_tree,\n",
    "                'right': right_tree}\n",
    "\n",
    "        return node\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self.gain(X, y, feature, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def gain(self, X, y, feature, threshold):\n",
    "        left_idxs = X[:, feature] < threshold\n",
    "        right_idxs = ~left_idxs\n",
    "\n",
    "        if np.sum(left_idxs) == 0 or np.sum(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        left_y = y[left_idxs]\n",
    "        right_y = y[right_idxs]\n",
    "\n",
    "        p_left = len(left_y) / len(y)\n",
    "        p_right = len(right_y) / len(y)\n",
    "\n",
    "        entropy_before_split = self.entropy(y)\n",
    "        entropy_after_split = (p_left * self.entropy(left_y)\n",
    "                               + p_right * self.entropy(right_y))\n",
    "\n",
    "        return entropy_before_split - entropy_after_split\n",
    "\n",
    "    def entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        p_i = counts / len(y)\n",
    "        \n",
    "        return -np.sum(p_i * np.log2(p_i))\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        if isinstance(node, int):\n",
    "            return node\n",
    "\n",
    "        if x[node['feature']] < node['threshold']:\n",
    "            return self.traverse_tree(x, node['left'])\n",
    "        \n",
    "        return self.traverse_tree(x, node['right'])\n",
    "```\n",
    "\n",
    "This implementation of the Decision Tree algorithm uses the `fit` method to train the model on the input data `X` and target labels `y`. The `predict` method can then be used to classify new instances of data based on the trained model. The `max_depth` parameter controls the maximum depth of the tree to prevent overfitting.\n",
    "\n",
    "The `build_tree` method recursively constructs the decision tree by finding the best split at each node based on the information gain criterion. The `find_best_split` method searches over all possible splits for each feature and returns the one with the highest information gain. The `gain` method calculates the information gain for a given split. The `entropy` method calculates the entropy of a set of labels.\n",
    "\n",
    "This implementation is generic and can be used with any dataset that can be loaded into a Pandas DataFrame.\n",
    "\n",
    "Prompt:\n",
    "Why are you using entropy gain? why not use gini?\n",
    "\n",
    "GPT - 4:\n",
    "The Decision Tree algorithm can use different criteria to measure the quality of a split. In this implementation, **entropy** is used as the criterion for splitting the nodes of the tree¹. Entropy is a measure of information that indicates the disorder of the features with respect to the target¹. The optimum split is chosen by the feature with less entropy¹.\n",
    "\n",
    "Another commonly used criterion is **Gini impurity**, which measures the frequency at which any element of the dataset will be mislabeled when it is randomly labeled¹. The Gini impurity is calculated using a different formula compared to entropy¹. The Gini Index has values inside the interval [0, 0.5], whereas the interval of entropy is [0, 1]¹. Both criteria have their own advantages and disadvantages.\n",
    "\n",
    "The choice between entropy and Gini impurity depends on various factors, including the specific problem, dataset characteristics, and personal preference¹. In practice, both criteria can yield similar results¹. The gini criterion is generally faster because it is less computationally expensive, while the entropy criterion may provide slightly better results¹.\n",
    "\n",
    "It's worth noting that this implementation uses entropy as a measure of information gain, but you can modify it to use Gini impurity or other criteria based on your specific requirements.\n",
    "\n",
    "Let me know if there's anything else I can help you with!\n",
    "\n",
    "Source: Conversation with Bing, 20/09/2023\n",
    "(1) Decision Trees: Gini vs Entropy | Quantdare. https://quantdare.com/decision-trees-gini-vs-entropy/.\n",
    "(2) ML | Gini Impurity and Entropy in Decision Tree - GeeksforGeeks. https://www.geeksforgeeks.org/gini-impurity-and-entropy-in-decision-tree-ml/.\n",
    "(3) Decision Tree with Gini index. If you don’t know about the Entropy .... https://purav-patel.medium.com/decision-tree-with-gini-index-4268f277817a.\n",
    "(4) When should I use Gini Impurity as opposed to Information Gain (Entropy)?. https://datascience.stackexchange.com/questions/10228/when-should-i-use-gini-impurity-as-opposed-to-information-gain-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DecisionTree:\n",
    "    ### Inicialização da árvore, definindo a profundidade máxima.\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    ### função para treinar o modelo\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    ### função para efetuar classificações após o modelo ter sido treinado\n",
    "    def predict(self, X):\n",
    "        return np.array([self.traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    ### função para construir a árvore. Como critério de parada, o chatgpt escolheu\n",
    "    ### ou a profundidade da árvore, ou número de classes = 1 (hardcoded),\n",
    "    ### ou número de sample = 2 (harcoded).\n",
    "    ### Ele busca a melhor divisão com uma função find_best_split, e faz uma\n",
    "    ### chamada recursiva para montar o resto da árvore.\n",
    "    ### Esta função está correta\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or n_classes == 1 or n_samples < 2):\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        # Find best split\n",
    "        best_feature, best_threshold = self.find_best_split(X, y)\n",
    "\n",
    "        # Split data\n",
    "        left_idxs = X[:, best_feature] < best_threshold\n",
    "        right_idxs = ~left_idxs\n",
    "        left_tree = self.build_tree(X[left_idxs], y[left_idxs], depth+1)\n",
    "        right_tree = self.build_tree(X[right_idxs], y[right_idxs], depth+1)\n",
    "\n",
    "        # Create node\n",
    "        node = {'feature': best_feature,\n",
    "                'threshold': best_threshold,\n",
    "                'left': left_tree,\n",
    "                'right': right_tree}\n",
    "\n",
    "        return node\n",
    "\n",
    "    ### função que encontra a melhor divisão deste nodo. Ela seleciona a melhor\n",
    "    ### divisão com base em um critério \"gain\", que é calculado em outra função.\n",
    "    ### A função está correta.\n",
    "    def find_best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self.gain(X, y, feature, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    ### Este foi o critério usado pelo chatgpt para escolher como fazer as divisões dos nodos. \n",
    "    ### \"gain\" é na verdade a diferença entre a entropia de antes e depois da divisão. Isso significa que o chatgpt está\n",
    "    ### escolhendo divisões que minimizam entropia, pois a fórmula: entropy_before_split - entropy_after_split\n",
    "    ### é maior quando 'entropy_after_split' é pequena, enquando a entropia antes da divisão é sempre a mesma.\n",
    "    ### Essa lógica está correta.\n",
    "    def gain(self, X, y, feature, threshold):\n",
    "        left_idxs = X[:, feature] < threshold\n",
    "        right_idxs = ~left_idxs\n",
    "\n",
    "        if np.sum(left_idxs) == 0 or np.sum(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        left_y = y[left_idxs]\n",
    "        right_y = y[right_idxs]\n",
    "\n",
    "        p_left = len(left_y) / len(y)\n",
    "        p_right = len(right_y) / len(y)\n",
    "\n",
    "        entropy_before_split = self.entropy(y)\n",
    "        entropy_after_split = (p_left * self.entropy(left_y)\n",
    "                               + p_right * self.entropy(right_y))\n",
    "\n",
    "        return entropy_before_split - entropy_after_split\n",
    "\n",
    "    ### Cálculo de entropia. O cálculo está correto.\n",
    "    def entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        p_i = counts / len(y)\n",
    "        \n",
    "        return -np.sum(p_i * np.log2(p_i))\n",
    "\n",
    "    ### \n",
    "    def traverse_tree(self, x, node):\n",
    "        ###Necessário mudar de int para ScalarType pois node nunca era int, mas int64. Com essa mudança, o código passou a inferir sem erros.\n",
    "        if isinstance(node, ScalarType):\n",
    "            return node\n",
    "\n",
    "        if x[node['feature']] < node['threshold']:\n",
    "            return self.traverse_tree(x, node['left'])\n",
    "\n",
    "        \n",
    "        return self.traverse_tree(x, node['right'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLEARN: Dataset  iris\n",
      "Accuracy: 1.0\n",
      "F1: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "ROC AUC score:  1.0\n",
      "------------------------------\n",
      "Accuracy: 1.0\n",
      "F1: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "ROC AUC score:  1.0\n",
      "\n",
      "SKLEARN: Dataset  wine\n",
      "Accuracy: 0.9722222222222222\n",
      "F1: 0.9777530589543938\n",
      "Precision: 0.9777777777777779\n",
      "Recall: 0.9791666666666666\n",
      "ROC AUC score:  0.9791666666666669\n",
      "------------------------------\n",
      "Accuracy: 0.9444444444444444\n",
      "F1: 0.9411799620132953\n",
      "Precision: 0.9315476190476191\n",
      "Recall: 0.9553571428571429\n",
      "ROC AUC score:  0.9637896825396824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (X, y), dataset in zip(datasets, datasets_names):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    print('SKLEARN: Dataset ', dataset)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('F1:', f1_score(y_test, y_pred, average='macro'))\n",
    "    print('Precision:', precision_score(y_test, y_pred, average='macro'))\n",
    "    print('Recall:', recall_score(y_test, y_pred, average='macro'))\n",
    "    print('ROC AUC score: ', roc_auc_score(y_test, tree.predict_proba(X_test), multi_class='ovr', average='micro'))\n",
    "\n",
    "    print('-'*30)\n",
    "\n",
    "    gpt_tree = DecisionTree()\n",
    "    gpt_tree.fit(X_train, y_train)\n",
    "    y_pred = gpt_tree.predict(X_test)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('F1:', f1_score(y_test, y_pred, average='macro'))\n",
    "    print('Precision:', precision_score(y_test, y_pred, average='macro'))\n",
    "    print('Recall:', recall_score(y_test, y_pred, average='macro'))\n",
    "    print('ROC AUC score: ', multiclass_roc_auc_score_gpt(y_test, y_pred))\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "\n",
    "> Act as a machine learning engineer. Write a Python script that only uses NumPy and Pandas and implements the KNN machine learning algorithm. The code should be commented, complete, and generic.\n",
    "\n",
    "Resposta:\n",
    "\n",
    "> Sure, here's a simple implementation of the K-Nearest Neighbors (KNN) algorithm using Python, NumPy, and Pandas. This script assumes that you have a dataset in a CSV file and that the last column of the dataset is the target variable.\n",
    "> \n",
    "> ```python\n",
    "> import numpy as np\n",
    "> import pandas as pd\n",
    "> from collections import Counter\n",
    "> \n",
    "> # Load dataset\n",
    "> def load_data(file_name):\n",
    ">     \"\"\"Load the CSV file and return a DataFrame.\"\"\"\n",
    ">     df = pd.read_csv(file_name)\n",
    ">     return df\n",
    "> \n",
    "> # Calculate Euclidean distance\n",
    "> def euclidean_distance(row1, row2):\n",
    ">     \"\"\"Calculate the Euclidean distance between two vectors.\"\"\"\n",
    ">     distance = np.sqrt(np.sum((row1 - row2)**2))\n",
    ">     return distance\n",
    "> \n",
    "> # Get nearest neighbors\n",
    "> def get_neighbors(train, test_row, num_neighbors):\n",
    ">     \"\"\"Get the most similar neighbors for a test instance.\"\"\"\n",
    ">     distances = list()\n",
    ">     for index, train_row in train.iterrows():\n",
    ">         dist = euclidean_distance(test_row[:-1], train_row[:-1])\n",
    ">         distances.append((train_row, dist))\n",
    ">     distances.sort(key=lambda tup: tup[1])\n",
    ">     neighbors = list()\n",
    ">     for i in range(num_neighbors):\n",
    ">         neighbors.append(distances[i][0])\n",
    ">     return neighbors\n",
    "> \n",
    "> # Make a prediction\n",
    "> def predict_classification(train, test_row, num_neighbors):\n",
    ">     \"\"\"Make a prediction with neighbors.\"\"\"\n",
    ">     neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    ">     output_values = [row[-1] for row in neighbors]\n",
    ">     prediction = max(set(output_values), key=output_values.count)\n",
    ">     return prediction\n",
    "> \n",
    "> # Test the function\n",
    "> def main():\n",
    ">     \"\"\"Load data, define number of neighbors and make a prediction.\"\"\"\n",
    ">     filename = 'your_file.csv'  # replace with your filename\n",
    ">     df = load_data(filename)\n",
    ">     num_neighbors = 3\n",
    ">     # assuming we want to predict for the first row in the dataset\n",
    ">     prediction = predict_classification(df, df.iloc[0], num_neighbors)\n",
    ">     print('Expected %s, Got %s.' % (df.iloc[0][-1], prediction))\n",
    "> \n",
    "> if __name__ == \"__main__\":\n",
    ">     main()\n",
    "> ```\n",
    "> \n",
    "> Please replace `'your_file.csv'` with your actual file name. This script uses Euclidean distance to calculate the similarity between instances (rows) of data. The `get_neighbors` function retrieves the `num_neighbors` most similar instances for a given test instance. The `predict_classification` function makes a prediction based on those neighbors. The `main` function loads data from a CSV file, defines a number of neighbors, and makes a prediction for the first row in the dataset.\n",
    "> \n",
    "> Remember to install any necessary libraries by running `pip install numpy pandas` in your terminal before executing this script.\n",
    "\n",
    "Usuário:\n",
    "> Your code doesn't consider every row, only the first one. Simulate Scikit-learn's implementation. Only send the updated code.\n",
    "\n",
    "GPT-4:\n",
    "> I understand your request. Here’s an updated version of the KNN algorithm that predicts for every row in the dataset, similar to Scikit-learn’s implementation:\n",
    ">\n",
    "> '''CODE'''\n",
    ">\n",
    "> This updated script makes a prediction for each row in the dataset by dropping the current row from the training set to avoid using it as its own neighbor. The predictions are stored in a list. Please replace 'your_file.csv' with your actual file name. Remember to install any necessary libraries by running pip install numpy pandas in your terminal before executing this script.\n",
    "\n",
    "Usuário:\n",
    "> You aren't dividing the dataset into two train/test datasets.\n",
    "\n",
    "GPT-4:\n",
    "> You’re right, I apologize for the oversight. Here’s the updated code that splits the dataset into a training set and a test set:\n",
    ">\n",
    "> '''CODE'''\n",
    ">\n",
    "> This script now uses train_test_split from sklearn.model_selection to split the dataset into a training set and a test set. The test_size parameter is set to 0.2, meaning that 20% of the data will be used for testing. The script then makes predictions for each row in the test set. Please replace 'your_file.csv' with your actual file name. Remember to install any necessary libraries by running pip install numpy pandas sklearn in your terminal before executing this script.\n",
    "\n",
    "Usuário:\n",
    "> Don't use sklearn. Remember what I said earlier\n",
    "\n",
    "GPT-4:\n",
    "> I apologize for misunderstanding your request earlier. Here’s the updated code that splits the dataset into a training set and a test set without using sklearn:\n",
    ">\n",
    "> '''CODE'''\n",
    ">\n",
    "> This script now uses a custom train_test_split function to split the dataset into a training set and a test set. The test_size parameter is set to 0.2, meaning that 20% of the data will be used for testing. The script then makes predictions for each row in the test set. Please replace 'your_file.csv' with your actual file name. Remember to install any necessary libraries by running pip install numpy pandas in your terminal before executing this script.\n",
    "\n",
    "Usuário:\n",
    "> Why are you importing Counter from collections? And you also aren't importing random to use its sample function. Also, explain in a few words the implementation of the predict_classification function.\n",
    "\n",
    "GPT-4:\n",
    "> You're correct, I apologize for the oversight. The `Counter` import is not necessary in this context, and the `random` module should indeed be imported to use its `sample` function. Here's the corrected import section:\n",
    "> \n",
    "> ```python\n",
    "> import numpy as np\n",
    "> import pandas as pd\n",
    "> import random\n",
    "> ```\n",
    "> \n",
    "> Regarding the `predict_classification` function, it works as follows:\n",
    "> \n",
    "> 1. It calls the `get_neighbors` function to find the `num_neighbors` closest neighbors to the `test_row`.\n",
    "> 2. It then extracts the output values (i.e., the classes or labels) of these neighbors.\n",
    "> 3. Finally, it returns the most common output value among these neighbors as the prediction for the `test_row`. If there's a tie for the most common output value, it selects one of them arbitrarily.\n",
    "> \n",
    "> This is a standard implementation of the K-Nearest Neighbors (KNN) algorithm, which classifies a data point based on how its neighbors are classified. The assumption is that similar data points (neighbors) have similar classifications. The number of neighbors considered (`num_neighbors`) is a hyperparameter that can be tuned for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usuário:\n",
    "> Give me a use-case to test this machine learning code\n",
    "\n",
    "GPT-4:\n",
    "> Sure, let's consider a use-case where we have a dataset of iris flowers. The dataset contains measurements of 150 iris flowers from three different species - Setosa, Versicolour, and Virginica. There are four features measured from each sample: the lengths and the widths of the sepals and petals.\n",
    "> \n",
    "> You can download the dataset from the UCI Machine Learning Repository. Here is the link to the dataset: `https://archive.ics.uci.edu/ml/datasets/iris`. The data is in a `.data` file, which you can open as a CSV file.\n",
    "> \n",
    "> Once you have downloaded the dataset, you can use it to test the KNN algorithm script. Replace `'your_file.csv'` in the script with the path to your downloaded iris dataset file. The script will then make predictions for each row in the test set (which is 20% of the data) based on the training set (which is 80% of the data).\n",
    "> \n",
    "> The KNN algorithm will classify each flower in the test set based on its closest neighbors in the training set. The output will be a list of predicted species for each flower in the test set.\n",
    "> \n",
    "> Please note that this is a simple implementation of KNN and may not give you the best accuracy as it doesn't include any data preprocessing steps like feature scaling or any parameter tuning. For more accurate results, consider using a machine learning library like scikit-learn which has optimized implementations of KNN and other algorithms.\n",
    "\n",
    "Usuário:\n",
    "> You may use sklearn datasets. Update your previous answer.\n",
    "\n",
    "GPT-4:\n",
    "> I apologize for the confusion. Here's the updated code that uses the iris dataset from `sklearn.datasets`:\n",
    "> \n",
    "> ```python\n",
    "> import numpy as np\n",
    "> import pandas as pd\n",
    "> import random\n",
    "> from sklearn.datasets import load_iris\n",
    "> \n",
    "> # Load dataset\n",
    "> def load_data():\n",
    ">     \"\"\"Load the iris dataset and return a DataFrame.\"\"\"\n",
    ">     iris = load_iris()\n",
    ">     df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    ">                      columns= iris['feature_names'] + ['target'])\n",
    ">     return df\n",
    "> \n",
    "> # Calculate Euclidean distance\n",
    "> def euclidean_distance(row1, row2):\n",
    ">     \"\"\"Calculate the Euclidean distance between two vectors.\"\"\"\n",
    ">     distance = np.sqrt(np.sum((row1 - row2)**2))\n",
    ">     return distance\n",
    "> \n",
    "> # Get nearest neighbors\n",
    "> def get_neighbors(train, test_row, num_neighbors):\n",
    ">     \"\"\"Get the most similar neighbors for a test instance.\"\"\"\n",
    ">     distances = list()\n",
    ">     for index, train_row in train.iterrows():\n",
    ">         dist = euclidean_distance(test_row[:-1], train_row[:-1])\n",
    ">         distances.append((train_row, dist))\n",
    ">     distances.sort(key=lambda tup: tup[1])\n",
    ">     neighbors = list()\n",
    ">     for i in range(num_neighbors):\n",
    ">         neighbors.append(distances[i][0])\n",
    ">     return neighbors\n",
    "> \n",
    "> # Make a prediction\n",
    "> def predict_classification(train, test_row, num_neighbors):\n",
    ">     \"\"\"Make a prediction with neighbors.\"\"\"\n",
    ">     neighbors = get_neighbors(train, test_row, num_neighbors)\n",
    ">     output_values = [row[-1] for row in neighbors]\n",
    ">     prediction = max(set(output_values), key=output_values.count)\n",
    ">     return prediction\n",
    "> \n",
    "> # Split dataset into training set and test set\n",
    "> def train_test_split(df, test_size):\n",
    ">     \"\"\"Split the DataFrame into training and testing sets.\"\"\"\n",
    ">     if isinstance(test_size, float):\n",
    ">         test_size = round(test_size * len(df))\n",
    "> \n",
    ">     indices = df.index.tolist()\n",
    ">     test_indices = random.sample(population=indices, k=test_size)\n",
    "> \n",
    ">     test_df = df.loc[test_indices]\n",
    ">     train_df = df.drop(test_indices)\n",
    ">     \n",
    ">     return train_df, test_df\n",
    "> \n",
    "> # Test the function\n",
    "> def main():\n",
    ">     \"\"\"Load data, define number of neighbors and make a prediction for each row.\"\"\"\n",
    ">     \n",
    ">     df = load_data()\n",
    ">     \n",
    ">     # split the dataset into training and testing sets\n",
    ">     train_df, test_df = train_test_split(df, test_size=0.2)\n",
    ">     \n",
    ">     num_neighbors = 3\n",
    ">     predictions = []\n",
    ">     \n",
    ">     # make a prediction for each row in the test set\n",
    ">     for index, row in test_df.iterrows():\n",
    ">         prediction = predict_classification(train_df, row, num_neighbors)\n",
    ">         predictions.append(prediction)\n",
    ">     \n",
    ">     print('Predictions:', predictions)\n",
    "> \n",
    "> if __name__ == \"__main__\":\n",
    ">     main()\n",
    "> ```\n",
    "> \n",
    "> This script now uses `load_iris` from `sklearn.datasets` to load the iris dataset directly. The `load_data` function has been updated accordingly. The rest of the script remains the same. Please remember to install any necessary libraries by running `pip install numpy pandas sklearn` in your terminal before executing this script.\n",
    "\n",
    "Usuário:\n",
    "> How would you verify the predictions?\n",
    "\n",
    "GPT-4:\n",
    "> To verify the predictions, you would compare them with the actual labels from the test set. You can calculate the accuracy of the predictions by counting the number of correct predictions and dividing by the total number of predictions. Here’s how you can do it:\n",
    "> \n",
    "> CODE\n",
    ">\n",
    "> This script now includes an accuracy_metric function that calculates the accuracy of the predictions. The main function has been updated to call this function after making predictions. It prints both the predictions and their accuracy. Please remember to install any necessary libraries by running pip install numpy pandas sklearn in your terminal before executing this script.\n",
    "\n",
    "Usuário:\n",
    "> I apologize for not having mentioned this earlier, but we're using a Jupyter notebook with every dependency already installed, so there's no need to remind me to install them every time.\n",
    "> Having said that, create a new, self-contained \"cell\" that evaluates your previous code. Implement the following metrics: accuracy, recall, precision, f1-score, and ROC. These functions should be generic, as they will be used to evaluate further models as well.\n",
    "\n",
    "GPT-4:\n",
    "> Hmm…let’s try a different topic. Sorry about that. What else is on your mind?\n",
    "\n",
    "Tentando mais uma vez...\n",
    "\n",
    "> Sorry! That’s on me, I can’t give a response to that right now. What else can I help you with?\n",
    "\n",
    "Parece que o GPT-4 perdeu o contexto da conversa.\n",
    "\n",
    "> Sure, here's a Python code snippet that calculates the accuracy, recall, precision, F1-score, and ROC AUC score. This code assumes that you have two lists: `y_true` containing the true labels and `y_pred` containing the predicted labels.\n",
    "> \n",
    "> ```python\n",
    "> import numpy as np\n",
    "> from sklearn import metrics\n",
    "> \n",
    "> # True binary labels\n",
    "> y_true = [0, 1, 1, 1, 0, 0]\n",
    "> \n",
    "> # Predicted binary labels\n",
    "> y_pred = [0, 0, 1, 1, 0, 1]\n",
    "> \n",
    "> def calculate_metrics(y_true, y_pred):\n",
    ">     # Accuracy\n",
    ">     accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    ">     print(f'Accuracy: {accuracy}')\n",
    "> \n",
    ">     # Recall\n",
    ">     recall = metrics.recall_score(y_true, y_pred)\n",
    ">     print(f'Recall: {recall}')\n",
    "> \n",
    ">     # Precision\n",
    ">     precision = metrics.precision_score(y_true, y_pred)\n",
    ">     print(f'Precision: {precision}')\n",
    "> \n",
    ">     # F1 Score\n",
    ">     f1 = metrics.f1_score(y_true, y_pred)\n",
    ">     print(f'F1 Score: {f1}')\n",
    "> \n",
    ">     # ROC AUC Score\n",
    ">     roc_auc = metrics.roc_auc_score(y_true, y_pred)\n",
    ">     print(f'ROC AUC Score: {roc_auc}')\n",
    "> \n",
    "> calculate_metrics(y_true, y_pred)\n",
    "> ```\n",
    "> \n",
    "> Please replace the `y_true` and `y_pred` lists with your actual data. The `calculate_metrics` function calculates and prints the five evaluation metrics. Note that these metrics are appropriate for binary classification problems. If you're working with multi-class classification problems, you might need to adjust the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "### OBERVAÇÃO: COMENTÁRIOS QUE COMEÇAM COM TRÊS '###' SÃO DO GRUPO\n",
    "\n",
    "\n",
    "### Função para carregar o dataset, ele coloca as features ao final dos dados.\n",
    "### A função está correta, mas força as features que desejamos classificar a estarem na última coluna dos dados.\n",
    "### Isso é consistente com as demais funções. Algumas delas simplesmente presumem que os dados estão nas últimas colunas.\n",
    "# Load dataset\n",
    "def load_data_knn_gpt(): ### renomeação nossa (load_data -> load_data_knn_gpt)\n",
    "    \"\"\"Load the iris dataset and return a DataFrame.\"\"\"\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "    return df\n",
    "\n",
    "### Função de calculo da distancia Euclidiana. A função está correta, desde que row1 e row2 sempre sejam vetores\n",
    "### de mesmo tamanho (do mesmo set de dados)\n",
    "# Calculate Euclidean distance\n",
    "def euclidean_distance_knn_gpt(row1, row2): ### renomeação nossa (euclidean_distance -> euclidean_distance_knn_gpt)\n",
    "    \"\"\"Calculate the Euclidean distance between two vectors.\"\"\"\n",
    "    distance = np.sqrt(np.sum((row1 - row2)**2))\n",
    "    return distance\n",
    "\n",
    "### Função de seleção dos K vizinhos. A estratégia usada foi calcular a distância de todos, guardar em uma lista, ordernar a lista\n",
    "### e selecionar os k primeiros elementos.\n",
    "### Logicamente, a função está correta, mas a etapa de ordernação é custosa e desnecessária. Ela terá complexidade O(nlog(n)), quando\n",
    "### uma estratégia mais inteligente poderia encontrar os k elementos menores com uma única passagem na lista de distâncias O(n)\n",
    "### Além disso, ele assume que a classe é a última coluna, o que pode não funcionar sempre.\n",
    "# Get nearest neighbors\n",
    "def get_neighbors_knn_gpt(train, test_row, num_neighbors): ### renomeação nossa (get_neighbors -> get_neighbors_knn_gpt)\n",
    "    \"\"\"Get the most similar neighbors for a test instance.\"\"\"\n",
    "    distances = list()\n",
    "    for index, train_row in train.iterrows():\n",
    "        dist = euclidean_distance_knn_gpt(test_row[:-1], train_row[:-1])\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = list()\n",
    "    for i in range(num_neighbors):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors\n",
    "\n",
    "### Função de inferência. Obtém os K vizinhos mais próximos através da função get_neighbors_knn_gpt, extrai as classes deles \n",
    "### e seleciona a classe com a maior ocorrência.\n",
    "### A função está correta, porém, assim como em get_neighbors_knn_gpt, o modelo assume que as classes estão na última coluna\n",
    "### o que pode não funcionar para todos os datasets.\n",
    "# Make a prediction\n",
    "def predict_classification_knn_gpt(train, test_row, num_neighbors): ### renomeação nossa (predict_classification -> predict_classification_knn_gpt)\n",
    "    \"\"\"Make a prediction with neighbors.\"\"\"\n",
    "    neighbors = get_neighbors_knn_gpt(train, test_row, num_neighbors)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "### Divide os datasets em treino e teste. A função está correta. É interessante que o ChatGPT resolveu aqui testar para garantir que test_size\n",
    "### é um inteiro.\n",
    "# Split dataset into training set and test set\n",
    "def train_test_split_knn_gpt(df, test_size): ### renomeação nossa (train_test_split -> train_test_split_knn_gpt)\n",
    "    \"\"\"Split the DataFrame into training and testing sets.\"\"\"\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "### Esta função foi modificada bastante por nós para nos auxiliar no teste do modelo. Ela devolve duas listas:\n",
    "### uma com as predições, outra com as classes corretas. O ChatGPT em sí não está calculando acurácia ou outras métricas,\n",
    "### mas com essas duas listas podemos efetuar os cálculos nós mesmos. O único problema desta função é o hardcoded num_neighbors.\n",
    "### O ideal seria que isso fosse um parâmetro da função, para que pudéssemos chamá-la facilmente com vários 'K's.\n",
    "# Test the function\n",
    "def knn_gpt(train_df, test_df): ### renomeação nossa (main -> knn_gpt), inclusão de parâmetros tb\n",
    "    \"\"\"Load data, define number of neighbors and make a prediction for each row.\"\"\"\n",
    "\n",
    "    # df = load_data_knn_gpt()\n",
    "\n",
    "    # # split the dataset into training and testing sets\n",
    "    # train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "    \n",
    "    num_neighbors = 3\n",
    "    predictions = []\n",
    "    truth = []\n",
    "    \n",
    "    # make a prediction for each row in the test set\n",
    "    for index, row in test_df.iterrows():\n",
    "        prediction = predict_classification_knn_gpt(train_df, row, num_neighbors)\n",
    "        predictions.append(prediction)\n",
    "        truth.append(row[-1])\n",
    "    \n",
    "    ### print('Predictions:', predictions) ### remoção nossa\n",
    "    \n",
    "    return predictions, truth ### adição nossa\n",
    "\n",
    "if __name__ == \"__main__\": ### atualizações nossas\n",
    "    df = load_data_knn_gpt()\n",
    "\n",
    "    # split the dataset into training and testing sets\n",
    "    train_df, test_df = train_test_split_knn_gpt(df, test_size=0.2)\n",
    "    knn_gpt(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos resultados\n",
    "\n",
    "Aqui comparamos os modelos escritos pelo GPT-4 com os modelos disponibilizados pela biblioteca scikit-learn.\n",
    "Os modelos da biblioteca scikit-learn não estão com seus parâmetros modificados, salvo para paridade com o código do GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelResult = Annotated[tuple[ArrayLike, ArrayLike, ScalarType], 'Model results as (y_pred, y_test, roc_auc_score)']\n",
    "\n",
    "def gpt_knn(X, y) -> ModelResult:\n",
    "    res = list()\n",
    "    for i, l in enumerate(X):\n",
    "        res.append(list(l) + [y[i]])\n",
    "\n",
    "    df = pd.DataFrame(data=res, columns=(f'col_{i}' for i in range(len(res[0]))))\n",
    "\n",
    "    # split the dataset into training and testing sets\n",
    "    random.seed(0)\n",
    "    train_df, test_df = train_test_split_knn_gpt(df, test_size=0.2)\n",
    "    y_pred, y_test = knn_gpt(train_df, test_df)\n",
    "\n",
    "    return y_pred, y_test, multiclass_roc_auc_score_gpt(y_test, y_pred)\n",
    "\n",
    "\n",
    "def sklearn_knn(X, y) -> ModelResult:\n",
    "    random.seed(0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return y_pred, y_test, roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr', average='micro')\n",
    "\n",
    "\n",
    "def gpt_decision_tree(X, y) -> ModelResult:\n",
    "    res = list()\n",
    "    for i, l in enumerate(X):\n",
    "        res.append(list(l) + [y[i]])\n",
    "\n",
    "    df = pd.DataFrame(data=res, columns=(f'col_{i}' for i in range(len(res[0]))))\n",
    "    \n",
    "    # split the dataset into training and testing sets\n",
    "    # p.s. we're doing this whole gymnastics routine to use gpt's own train/test split function.\n",
    "    #      we could simply use sklearn's implementation and we'd be good to go, but for sake of\n",
    "    #      completion, we're using gpt-4's.\n",
    "    random.seed(0)\n",
    "    train_df, test_df = train_test_split_gpt(df, 0.2)\n",
    "    X_train = train_df.iloc[:, :-1]\n",
    "    y_train = train_df.iloc[:, -1]\n",
    "    X_test = test_df.iloc[:, :-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "\n",
    "    gpt_tree = DecisionTree()\n",
    "    gpt_tree.fit(X_train, y_train)\n",
    "    y_pred = gpt_tree.predict(X_test)\n",
    "    return y_pred, y_test, multiclass_roc_auc_score_gpt(y_test, y_pred)\n",
    "\n",
    "\n",
    "def sklearn_decision_tree(X, y) -> ModelResult:\n",
    "    random.seed(0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return y_pred, y_test, roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr', average='micro')\n",
    "\n",
    "def gpt_naive_bayes(X, y) -> ModelResult: ...\n",
    "\n",
    "\n",
    "def sklearn_naive_bayes(X, y) -> ModelResult:\n",
    "    random.seed(0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return y_pred, y_test, roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr', average='micro')\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Decision tree': {\n",
    "        'gpt': gpt_decision_tree,\n",
    "        'sklearn': sklearn_decision_tree,\n",
    "    },\n",
    "    'KNN': {\n",
    "        'gpt': gpt_knn,\n",
    "        'sklearn': sklearn_knn,\n",
    "    },\n",
    "    'Naïve Bayes': {\n",
    "        'gpt': lambda *_: (None, None, None),\n",
    "        'sklearn': sklearn_naive_bayes,\n",
    "    },\n",
    "}\n",
    "\n",
    "colors = [plt.cm.Blues, plt.cm.Oranges]\n",
    "\n",
    "model_results: list[dict] = list()\n",
    "for model_name, model_impl in models.items():\n",
    "    fig, axes = plt.subplots(nrows=len(datasets), ncols=2, figsize=(10, 10))\n",
    "    fig.tight_layout(pad=2, h_pad=4)\n",
    "\n",
    "    i = 0\n",
    "    for (X, y), dataset in zip(datasets, datasets_names):\n",
    "        j = 0\n",
    "        for author, implementation in model_impl.items():\n",
    "            y_pred, y_test, _roc_auc_score = implementation(X, y)\n",
    "            model_results.append(get_generic_metrics(author, model_name, dataset, y_pred, y_test, _roc_auc_score))\n",
    "\n",
    "            if _roc_auc_score == None:\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            display_confusion_matrix = ConfusionMatrixDisplay(cm)\n",
    "            display_confusion_matrix.plot(ax=axes[i][j], cmap=colors[j])\n",
    "            axes[i][j].set_title(f'{author} {model_name}: {dataset}')\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    display(fig)\n",
    "\n",
    "res_df = pd.DataFrame.from_records(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt</td>\n",
       "      <td>Decision tree</td>\n",
       "      <td>iris</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>Decision tree</td>\n",
       "      <td>iris</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt</td>\n",
       "      <td>Decision tree</td>\n",
       "      <td>wine</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.941180</td>\n",
       "      <td>0.955357</td>\n",
       "      <td>0.931548</td>\n",
       "      <td>0.963790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>Decision tree</td>\n",
       "      <td>wine</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.977753</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt</td>\n",
       "      <td>KNN</td>\n",
       "      <td>iris</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.967963</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.978175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>KNN</td>\n",
       "      <td>iris</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.961026</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.999444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt</td>\n",
       "      <td>KNN</td>\n",
       "      <td>wine</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.646888</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.663919</td>\n",
       "      <td>0.725173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>KNN</td>\n",
       "      <td>wine</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.711729</td>\n",
       "      <td>0.712302</td>\n",
       "      <td>0.713889</td>\n",
       "      <td>0.892747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt</td>\n",
       "      <td>Naïve Bayes</td>\n",
       "      <td>iris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>Naïve Bayes</td>\n",
       "      <td>iris</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.957351</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.999444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpt</td>\n",
       "      <td>Naïve Bayes</td>\n",
       "      <td>wine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sklearn</td>\n",
       "      <td>Naïve Bayes</td>\n",
       "      <td>wine</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.917654</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.996528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author          model dataset  accuracy        f1  precision    recall  \\\n",
       "0       gpt  Decision tree    iris  1.000000  1.000000   1.000000  1.000000   \n",
       "1   sklearn  Decision tree    iris  1.000000  1.000000   1.000000  1.000000   \n",
       "2       gpt  Decision tree    wine  0.944444  0.941180   0.955357  0.931548   \n",
       "3   sklearn  Decision tree    wine  0.972222  0.977753   0.979167  0.977778   \n",
       "4       gpt            KNN    iris  0.966667  0.967963   0.972222  0.966667   \n",
       "5   sklearn            KNN    iris  0.966667  0.961026   0.974359  0.952381   \n",
       "6       gpt            KNN    wine  0.638889  0.646888   0.638889  0.663919   \n",
       "7   sklearn            KNN    wine  0.805556  0.711729   0.712302  0.713889   \n",
       "8       gpt    Naïve Bayes    iris       NaN       NaN        NaN       NaN   \n",
       "9   sklearn    Naïve Bayes    iris  0.966667  0.957351   0.944444  0.976190   \n",
       "10      gpt    Naïve Bayes    wine       NaN       NaN        NaN       NaN   \n",
       "11  sklearn    Naïve Bayes    wine  0.916667  0.917654   0.937500  0.910714   \n",
       "\n",
       "     roc_auc  \n",
       "0   1.000000  \n",
       "1   1.000000  \n",
       "2   0.963790  \n",
       "3   0.979167  \n",
       "4   0.978175  \n",
       "5   0.999444  \n",
       "6   0.725173  \n",
       "7   0.892747  \n",
       "8        NaN  \n",
       "9   0.999444  \n",
       "10       NaN  \n",
       "11  0.996528  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
